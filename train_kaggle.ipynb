{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14262980,"sourceType":"datasetVersion","datasetId":9101353}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MOF Crystal Diffusion VAE Training\n\nThis notebook trains a Crystal Diffusion VAE model on MOF (Metal-Organic Framework) structures.\n\n## Setup\n- Device: CUDA (GPU) on Kaggle\n- Batch Size: 16 (effective: 128 with gradient accumulation)\n- Architecture: Crystal Diffusion VAE with GNN encoder/decoder\n","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Raiden-Makoto/IngestAMOF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:11:46.859919Z","iopub.execute_input":"2025-12-23T04:11:46.860092Z","iopub.status.idle":"2025-12-23T04:11:47.492130Z","shell.execute_reply.started":"2025-12-23T04:11:46.860073Z","shell.execute_reply":"2025-12-23T04:11:47.491244Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'IngestAMOF'...\nremote: Enumerating objects: 75, done.\u001b[K\nremote: Counting objects: 100% (75/75), done.\u001b[K\nremote: Compressing objects: 100% (57/57), done.\u001b[K\nremote: Total 75 (delta 23), reused 61 (delta 15), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (75/75), 32.55 KiB | 1.36 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd IngestAMOF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:12:37.222919Z","iopub.execute_input":"2025-12-23T04:12:37.223711Z","iopub.status.idle":"2025-12-23T04:12:37.229189Z","shell.execute_reply.started":"2025-12-23T04:12:37.223671Z","shell.execute_reply":"2025-12-23T04:12:37.228538Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/IngestAMOF\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip -q install pymatgen","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:14:04.454548Z","iopub.execute_input":"2025-12-23T04:14:04.455283Z","iopub.status.idle":"2025-12-23T04:14:13.628674Z","shell.execute_reply.started":"2025-12-23T04:14:04.455252Z","shell.execute_reply":"2025-12-23T04:14:13.627967Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport os\nimport sys\n\n# Add current directory to path for imports (Kaggle structure)\nif '/kaggle/working' not in sys.path:\n    sys.path.insert(0, '/kaggle/working')\nif '.' not in sys.path:\n    sys.path.insert(0, '.')\n\nfrom utils.dataloader import get_dataloader, MOFDataset, collate_mols\nfrom torch.utils.data import DataLoader, random_split\nfrom model.cdvae import CrystalDiffusionVAE\n\nprint(\"âœ“ Imports successful\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:14:26.763561Z","iopub.execute_input":"2025-12-23T04:14:26.764260Z","iopub.status.idle":"2025-12-23T04:14:26.777419Z","shell.execute_reply.started":"2025-12-23T04:14:26.764197Z","shell.execute_reply":"2025-12-23T04:14:26.776741Z"}},"outputs":[{"name":"stdout","text":"âœ“ Imports successful\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Configuration\n\nSet hyperparameters and paths for Kaggle environment.\n","metadata":{}},{"cell_type":"code","source":"# --- HYPERPARAMETERS ---\n# Hardware / System\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nCHECKPOINT_DIR = \"./checkpoints\"  # Kaggle working directory\n\n# Training Config\nBATCH_SIZE = 16        # Safe for GPU memory\nGRAD_ACCUM_STEPS = 8   # Accumulate gradients over 8 batches (effective batch size = 16 * 8 = 128)\nLR = 1e-3              # Learning Rate\nEPOCHS = 50           # Total passes through data\nKL_WEIGHT = 1.0       # Weight for KL Divergence (keeps latent space neat)\nGRAD_CLIP = 1.0        # Prevents exploding gradients\n\n# Model Architecture\nHIDDEN_DIM = 64\nLATENT_DIM = 64\nNUM_LAYERS = 2\nTIMESTEPS = 1000\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n\n# Create checkpoint directory\nif not os.path.exists(CHECKPOINT_DIR):\n    os.makedirs(CHECKPOINT_DIR)\n    \nprint(f\"ğŸš€ Starting MOF-Diffusion Training\")\nprint(f\"   Device:      {DEVICE}\")\nprint(f\"   Batch Size:  {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS} with gradient accumulation)\")\nprint(f\"   Latent Dim:  {LATENT_DIM}\")\nprint(\"=\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:26:00.025294Z","iopub.execute_input":"2025-12-23T04:26:00.026116Z","iopub.status.idle":"2025-12-23T04:26:00.033348Z","shell.execute_reply.started":"2025-12-23T04:26:00.026078Z","shell.execute_reply":"2025-12-23T04:26:00.032635Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nCUDA Available: True\nGPU: Tesla T4\nğŸš€ Starting MOF-Diffusion Training\n   Device:      cuda\n   Batch Size:  16 (effective: 128 with gradient accumulation)\n   Latent Dim:  64\n============================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Setup Environment\n","metadata":{}},{"cell_type":"markdown","source":"## Load and Split Data\n\nLoad the dataset and split into 80% training and 20% test (test is not used for training).\n","metadata":{}},{"cell_type":"code","source":"# Load Data and Split (80% train, 20% test - but we only use train)\n# Adjust DATA_DIR based on where your processed_graphs folder is located\nDATA_DIR = \"../../input/processed-graphs\"  # Or \"/kaggle/working/processed_graphs\"\n\nfull_dataset = MOFDataset(DATA_DIR)\ntotal_size = len(full_dataset)\ntrain_size = int(0.8 * total_size)\ntest_size = total_size - train_size\n\n# Split dataset\ntrain_dataset, test_dataset = random_split(\n    full_dataset, \n    [train_size, test_size],\n    generator=torch.Generator().manual_seed(42)  # For reproducibility\n)\n\n# Create dataloader only for training data\ndataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_mols,\n    num_workers=0,\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"ğŸ“š Data Loaded: {total_size} total crystals\")\nprint(f\"   Training: {train_size} crystals (80%)\")\nprint(f\"   Test: {test_size} crystals (20% - not used for training)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:26:03.670744Z","iopub.execute_input":"2025-12-23T04:26:03.671423Z","iopub.status.idle":"2025-12-23T04:26:03.689538Z","shell.execute_reply.started":"2025-12-23T04:26:03.671394Z","shell.execute_reply":"2025-12-23T04:26:03.688958Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded: 878 crystals found.\nğŸ“š Data Loaded: 878 total crystals\n   Training: 702 crystals (80%)\n   Test: 176 crystals (20% - not used for training)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Initialize Model and Optimizer\n","metadata":{}},{"cell_type":"code","source":"# Initialize Model\nmodel = CrystalDiffusionVAE(\n    hidden_dim=HIDDEN_DIM, \n    latent_dim=LATENT_DIM, \n    num_layers=NUM_LAYERS,\n    num_timesteps=TIMESTEPS\n).to(DEVICE)\n\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\nprint(f\"âœ“ Model initialized on {DEVICE}\")\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:26:06.228883Z","iopub.execute_input":"2025-12-23T04:26:06.229149Z","iopub.status.idle":"2025-12-23T04:26:06.245058Z","shell.execute_reply.started":"2025-12-23T04:26:06.229128Z","shell.execute_reply":"2025-12-23T04:26:06.244347Z"}},"outputs":[{"name":"stdout","text":"âœ“ Model initialized on cuda\n   Parameters: 162,599\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Training Loop\n","metadata":{}},{"cell_type":"code","source":"# Training Loop\nmodel.train()\n\nEPOCHS=150\n\nfor epoch in range(EPOCHS):\n    total_recon_loss = 0\n    total_kl_loss = 0\n    batch_count = 0\n    \n    # Progress Bar for this Epoch\n    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for batch_idx, batch in enumerate(pbar):\n        # --- MEMORY FIX 1: Use set_to_none=True to delete grads instead of zeroing ---\n        optimizer.zero_grad(set_to_none=True)  # SAVES MEMORY: deletes grads instead of zeroing them\n        # Move Data to Device\n        atom_types = batch['atom_types'].to(DEVICE)\n        frac_coords = batch['frac_coords'].to(DEVICE)\n        lattice = batch['lattice'].to(DEVICE)\n        mask = batch['mask'].to(DEVICE)\n        \n        # --- FORWARD PASS ---\n        # Model handles encoding, sampling z, adding noise, and decoding\n        pred_noise, target_noise, mu, log_var = model(\n            atom_types, frac_coords, lattice, mask\n        )\n        \n        # --- LOSS CALCULATION ---\n        # 1. Reconstruction Loss (MSE between Predicted Noise and Real Noise)\n        # We must normalize by the number of valid atoms (sum of mask)\n        # Note: The output is already masked inside the model, but we double-check mask sum\n        num_valid_atoms = torch.sum(mask)\n        recon_loss = F.mse_loss(pred_noise, target_noise, reduction='sum') / (num_valid_atoms + 1e-6)\n        \n        # 2. KL Divergence (Regularization)\n        # Analytical KL for Normal Distributions\n        # sum(1 + log(var) - mu^2 - var)\n        kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        # Normalize by batch size and apply weight\n        kl_loss = (kld / atom_types.size(0)) * KL_WEIGHT\n        \n        # Total Loss - scale by accumulation steps to average over accumulated batches\n        loss = (recon_loss + kl_loss) / GRAD_ACCUM_STEPS\n        \n        # --- BACKPROPAGATION (Accumulate Gradients) ---\n        loss.backward()\n        \n        # --- LOGGING ---\n        # Store loss values before deletion for logging\n        recon_loss_val = recon_loss.item()\n        kl_loss_val = kl_loss.item()\n        total_recon_loss += recon_loss_val\n        total_kl_loss += kl_loss_val\n        batch_count += 1\n        \n        # Update every GRAD_ACCUM_STEPS batches or at the end of epoch\n        is_accumulation_step = (batch_idx + 1) % GRAD_ACCUM_STEPS == 0\n        is_last_batch = (batch_idx + 1) == len(dataloader)\n        \n        if is_accumulation_step or is_last_batch:\n            # Clip Gradients (Crucial for GNN stability)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n            \n            # Take optimizer step (this uses accumulated gradients)\n            optimizer.step()\n        \n        # --- MEMORY FIX 2: DELETE TENSORS ---\n        # Explicitly delete heavy variables to free graph references\n        del pred_noise, target_noise, mu, log_var, loss, recon_loss, kl_loss, kld\n        \n        # --- MEMORY FIX 3: Clear CUDA cache (for GPU) ---\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        # Update Progress Bar (show unscaled losses)\n        pbar.set_postfix({\n            'Recon': f\"{recon_loss_val:.4f}\", \n            'KL': f\"{kl_loss_val:.4f}\",\n            'Accum': f\"{(batch_idx + 1) % GRAD_ACCUM_STEPS}/{GRAD_ACCUM_STEPS}\"\n        })\n    \n    # End of Epoch Summary\n    avg_recon = total_recon_loss / batch_count\n    avg_kl = total_kl_loss / batch_count\n    avg_total_loss = avg_recon + avg_kl\n    print(f\"   Done. Avg Recon: {avg_recon:.4f} | Avg KL: {avg_kl:.4f} | Avg Total Loss: {avg_total_loss:.4f}\")\n    \n    # --- CHECKPOINTING ---\n    # Save model every 10 epochs (and the first one)\n    if (epoch + 1) % 10 == 0 or epoch == 0:\n        save_path = os.path.join(CHECKPOINT_DIR, f\"cdvae_epoch_{epoch+1}.pt\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_total_loss,\n        }, save_path)\n        print(f\"   ğŸ’¾ Saved checkpoint to {save_path}\")\n\nprint(\"\\nâœ… Training Complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:26:11.871666Z","iopub.execute_input":"2025-12-23T04:26:11.871961Z","iopub.status.idle":"2025-12-23T04:27:30.307106Z","shell.execute_reply.started":"2025-12-23T04:26:11.871935Z","shell.execute_reply":"2025-12-23T04:27:30.306428Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:08<00:00,  5.16it/s, Recon=2.8846, KL=1.9075, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0616 | Avg KL: 5.3816 | Avg Total Loss: 8.4432\n   ğŸ’¾ Saved checkpoint to ./checkpoints/cdvae_epoch_1.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.52it/s, Recon=3.0199, KL=0.6124, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0106 | Avg KL: 0.8498 | Avg Total Loss: 3.8604\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.61it/s, Recon=3.0220, KL=0.2723, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0091 | Avg KL: 0.3320 | Avg Total Loss: 3.3411\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.70it/s, Recon=2.9160, KL=0.1245, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0191 | Avg KL: 0.1927 | Avg Total Loss: 3.2118\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.66it/s, Recon=3.0336, KL=0.0935, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 2.9994 | Avg KL: 0.1154 | Avg Total Loss: 3.1148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.72it/s, Recon=2.9056, KL=0.0541, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0175 | Avg KL: 0.0729 | Avg Total Loss: 3.0905\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.75it/s, Recon=2.9716, KL=0.0317, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0058 | Avg KL: 0.0541 | Avg Total Loss: 3.0599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.69it/s, Recon=3.0856, KL=0.0277, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0128 | Avg KL: 0.0404 | Avg Total Loss: 3.0532\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.72it/s, Recon=3.0082, KL=0.0278, Accum=4/8]\n","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 3.0012 | Avg KL: 0.0316 | Avg Total Loss: 3.0328\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.67it/s, Recon=2.9777, KL=0.0176, Accum=4/8]","output_type":"stream"},{"name":"stdout","text":"   Done. Avg Recon: 2.9994 | Avg KL: 0.0245 | Avg Total Loss: 3.0239\n   ğŸ’¾ Saved checkpoint to ./checkpoints/cdvae_epoch_10.pt\n\nâœ… Training Complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Training Complete!\n\nCheckpoints are saved in `/kaggle/working/checkpoints/`. These will be available for download after the notebook run completes.\n","metadata":{}},{"cell_type":"code","source":"!ls -l | grep \"check\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:40:00.898069Z","iopub.execute_input":"2025-12-23T04:40:00.898834Z","iopub.status.idle":"2025-12-23T04:40:01.035810Z","shell.execute_reply.started":"2025-12-23T04:40:00.898801Z","shell.execute_reply":"2025-12-23T04:40:01.035093Z"}},"outputs":[{"name":"stdout","text":"drwxr-xr-x 2 root root  4096 Dec 23 04:27 checkpoints\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!cd checkpoints && ls -a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T04:40:24.326236Z","iopub.execute_input":"2025-12-23T04:40:24.326844Z","iopub.status.idle":"2025-12-23T04:40:24.462613Z","shell.execute_reply.started":"2025-12-23T04:40:24.326806Z","shell.execute_reply":"2025-12-23T04:40:24.461712Z"}},"outputs":[{"name":"stdout","text":".  ..  cdvae_epoch_10.pt  cdvae_epoch_1.pt\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}