{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOF Crystal Diffusion VAE Training\n",
    "\n",
    "This notebook trains a Crystal Diffusion VAE model on MOF (Metal-Organic Framework) structures.\n",
    "\n",
    "## Setup\n",
    "- Device: CUDA (GPU) on Kaggle\n",
    "- Batch Size: 16 (effective: 128 with gradient accumulation)\n",
    "- Architecture: Crystal Diffusion VAE with GNN encoder/decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:11:46.860092Z",
     "iopub.status.busy": "2025-12-23T04:11:46.859919Z",
     "iopub.status.idle": "2025-12-23T04:11:47.492130Z",
     "shell.execute_reply": "2025-12-23T04:11:47.491244Z",
     "shell.execute_reply.started": "2025-12-23T04:11:46.860073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IngestAMOF'...\n",
      "remote: Enumerating objects: 75, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 75 (delta 23), reused 61 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (75/75), 32.55 KiB | 1.36 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Raiden-Makoto/IngestAMOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:12:37.223711Z",
     "iopub.status.busy": "2025-12-23T04:12:37.222919Z",
     "iopub.status.idle": "2025-12-23T04:12:37.229189Z",
     "shell.execute_reply": "2025-12-23T04:12:37.228538Z",
     "shell.execute_reply.started": "2025-12-23T04:12:37.223671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/IngestAMOF\n"
     ]
    }
   ],
   "source": [
    "%cd IngestAMOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:14:04.455283Z",
     "iopub.status.busy": "2025-12-23T04:14:04.454548Z",
     "iopub.status.idle": "2025-12-23T04:14:13.628674Z",
     "shell.execute_reply": "2025-12-23T04:14:13.627967Z",
     "shell.execute_reply.started": "2025-12-23T04:14:04.455252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip -q install pymatgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:14:26.764260Z",
     "iopub.status.busy": "2025-12-23T04:14:26.763561Z",
     "iopub.status.idle": "2025-12-23T04:14:26.777419Z",
     "shell.execute_reply": "2025-12-23T04:14:26.776741Z",
     "shell.execute_reply.started": "2025-12-23T04:14:26.764197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add current directory to path for imports (Kaggle structure)\n",
    "if '/kaggle/working' not in sys.path:\n",
    "    sys.path.insert(0, '/kaggle/working')\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.')\n",
    "\n",
    "from utils.dataloader import get_dataloader, MOFDataset, collate_mols\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from model.cdvae import CrystalDiffusionVAE\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set hyperparameters and paths for Kaggle environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:26:00.026116Z",
     "iopub.status.busy": "2025-12-23T04:26:00.025294Z",
     "iopub.status.idle": "2025-12-23T04:26:00.033348Z",
     "shell.execute_reply": "2025-12-23T04:26:00.032635Z",
     "shell.execute_reply.started": "2025-12-23T04:26:00.026078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA Available: True\n",
      "GPU: Tesla T4\n",
      "ğŸš€ Starting MOF-Diffusion Training\n",
      "   Device:      cuda\n",
      "   Batch Size:  16 (effective: 128 with gradient accumulation)\n",
      "   Latent Dim:  64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "# Hardware / System\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_DIR = \"./checkpoints\"  # Kaggle working directory\n",
    "\n",
    "# Training Config\n",
    "BATCH_SIZE = 16        # Safe for GPU memory\n",
    "GRAD_ACCUM_STEPS = 8   # Accumulate gradients over 8 batches (effective batch size = 16 * 8 = 128)\n",
    "LR = 1e-3              # Learning Rate\n",
    "EPOCHS = 50           # Total passes through data\n",
    "KL_WEIGHT = 1.0       # Weight for KL Divergence (keeps latent space neat)\n",
    "GRAD_CLIP = 1.0        # Prevents exploding gradients\n",
    "\n",
    "# Model Architecture\n",
    "HIDDEN_DIM = 64\n",
    "LATENT_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "    \n",
    "print(f\"ğŸš€ Starting MOF-Diffusion Training\")\n",
    "print(f\"   Device:      {DEVICE}\")\n",
    "print(f\"   Batch Size:  {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS} with gradient accumulation)\")\n",
    "print(f\"   Latent Dim:  {LATENT_DIM}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data\n",
    "\n",
    "Load the dataset and split into 80% training and 20% test (test is not used for training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:26:03.671423Z",
     "iopub.status.busy": "2025-12-23T04:26:03.670744Z",
     "iopub.status.idle": "2025-12-23T04:26:03.689538Z",
     "shell.execute_reply": "2025-12-23T04:26:03.688958Z",
     "shell.execute_reply.started": "2025-12-23T04:26:03.671394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 878 crystals found.\n",
      "ğŸ“š Data Loaded: 878 total crystals\n",
      "   Training: 702 crystals (80%)\n",
      "   Test: 176 crystals (20% - not used for training)\n"
     ]
    }
   ],
   "source": [
    "# Load Data and Split (80% train, 20% test - but we only use train)\n",
    "# Adjust DATA_DIR based on where your processed_graphs folder is located\n",
    "DATA_DIR = \"../../input/processed-graphs\"  # Or \"/kaggle/working/processed_graphs\"\n",
    "\n",
    "full_dataset = MOFDataset(DATA_DIR)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")\n",
    "\n",
    "# Create dataloader only for training data\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mols,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“š Data Loaded: {total_size} total crystals\")\n",
    "print(f\"   Training: {train_size} crystals (80%)\")\n",
    "print(f\"   Test: {test_size} crystals (20% - not used for training)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:26:06.229149Z",
     "iopub.status.busy": "2025-12-23T04:26:06.228883Z",
     "iopub.status.idle": "2025-12-23T04:26:06.245058Z",
     "shell.execute_reply": "2025-12-23T04:26:06.244347Z",
     "shell.execute_reply.started": "2025-12-23T04:26:06.229128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model initialized on cuda\n",
      "   Parameters: 162,599\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "model = CrystalDiffusionVAE(\n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    latent_dim=LATENT_DIM, \n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_timesteps=TIMESTEPS\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"âœ“ Model initialized on {DEVICE}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Training with early stopping:\n",
    "- **Warmup**: 100 epochs before early stopping checks begin\n",
    "- **Patience**: 10 epochs without improvement triggers early stop\n",
    "- **Best model**: Automatically saved when new best loss is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:26:11.871961Z",
     "iopub.status.busy": "2025-12-23T04:26:11.871666Z",
     "iopub.status.idle": "2025-12-23T04:27:30.307106Z",
     "shell.execute_reply": "2025-12-23T04:27:30.306428Z",
     "shell.execute_reply.started": "2025-12-23T04:26:11.871935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:08<00:00,  5.16it/s, Recon=2.8846, KL=1.9075, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0616 | Avg KL: 5.3816 | Avg Total Loss: 8.4432\n",
      "   ğŸ’¾ Saved checkpoint to ./checkpoints/cdvae_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.52it/s, Recon=3.0199, KL=0.6124, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0106 | Avg KL: 0.8498 | Avg Total Loss: 3.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.61it/s, Recon=3.0220, KL=0.2723, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0091 | Avg KL: 0.3320 | Avg Total Loss: 3.3411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.70it/s, Recon=2.9160, KL=0.1245, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0191 | Avg KL: 0.1927 | Avg Total Loss: 3.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.66it/s, Recon=3.0336, KL=0.0935, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 2.9994 | Avg KL: 0.1154 | Avg Total Loss: 3.1148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.72it/s, Recon=2.9056, KL=0.0541, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0175 | Avg KL: 0.0729 | Avg Total Loss: 3.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.75it/s, Recon=2.9716, KL=0.0317, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0058 | Avg KL: 0.0541 | Avg Total Loss: 3.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.69it/s, Recon=3.0856, KL=0.0277, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0128 | Avg KL: 0.0404 | Avg Total Loss: 3.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.72it/s, Recon=3.0082, KL=0.0278, Accum=4/8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 3.0012 | Avg KL: 0.0316 | Avg Total Loss: 3.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:07<00:00,  5.67it/s, Recon=2.9777, KL=0.0176, Accum=4/8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Done. Avg Recon: 2.9994 | Avg KL: 0.0245 | Avg Total Loss: 3.0239\n",
      "   ğŸ’¾ Saved checkpoint to ./checkpoints/cdvae_epoch_10.pt\n",
      "\n",
      "âœ… Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Early Stopping\n",
    "model.train()\n",
    "\n",
    "EPOCHS = 150\n",
    "EARLY_STOP_PATIENCE = 10  # Stop if no improvement for 10 epochs\n",
    "EARLY_STOP_WARMUP = 100   # Wait 100 epochs before starting early stopping check\n",
    "\n",
    "# Early stopping variables\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, \"cdvae_best.pt\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Progress Bar for this Epoch\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # --- MEMORY FIX 1: Use set_to_none=True to delete grads instead of zeroing ---\n",
    "        optimizer.zero_grad(set_to_none=True)  # SAVES MEMORY: deletes grads instead of zeroing them\n",
    "        # Move Data to Device\n",
    "        atom_types = batch['atom_types'].to(DEVICE)\n",
    "        frac_coords = batch['frac_coords'].to(DEVICE)\n",
    "        lattice = batch['lattice'].to(DEVICE)\n",
    "        mask = batch['mask'].to(DEVICE)\n",
    "        \n",
    "        # --- FORWARD PASS ---\n",
    "        # Model handles encoding, sampling z, adding noise, and decoding\n",
    "        pred_noise, target_noise, mu, log_var = model(\n",
    "            atom_types, frac_coords, lattice, mask\n",
    "        )\n",
    "        \n",
    "        # --- LOSS CALCULATION ---\n",
    "        # 1. Reconstruction Loss (MSE between Predicted Noise and Real Noise)\n",
    "        # We must normalize by the number of valid atoms (sum of mask)\n",
    "        # Note: The output is already masked inside the model, but we double-check mask sum\n",
    "        num_valid_atoms = torch.sum(mask)\n",
    "        recon_loss = F.mse_loss(pred_noise, target_noise, reduction='sum') / (num_valid_atoms + 1e-6)\n",
    "        \n",
    "        # 2. KL Divergence (Regularization)\n",
    "        # Analytical KL for Normal Distributions\n",
    "        # sum(1 + log(var) - mu^2 - var)\n",
    "        kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # Normalize by batch size and apply weight\n",
    "        kl_loss = (kld / atom_types.size(0)) * KL_WEIGHT\n",
    "        \n",
    "        # Total Loss - scale by accumulation steps to average over accumulated batches\n",
    "        loss = (recon_loss + kl_loss) / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        # --- BACKPROPAGATION (Accumulate Gradients) ---\n",
    "        loss.backward()\n",
    "        \n",
    "        # --- LOGGING ---\n",
    "        # Store loss values before deletion for logging\n",
    "        recon_loss_val = recon_loss.item()\n",
    "        kl_loss_val = kl_loss.item()\n",
    "        total_recon_loss += recon_loss_val\n",
    "        total_kl_loss += kl_loss_val\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Update every GRAD_ACCUM_STEPS batches or at the end of epoch\n",
    "        is_accumulation_step = (batch_idx + 1) % GRAD_ACCUM_STEPS == 0\n",
    "        is_last_batch = (batch_idx + 1) == len(dataloader)\n",
    "        \n",
    "        if is_accumulation_step or is_last_batch:\n",
    "            # Clip Gradients (Crucial for GNN stability)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            \n",
    "            # Take optimizer step (this uses accumulated gradients)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # --- MEMORY FIX 2: DELETE TENSORS ---\n",
    "        # Explicitly delete heavy variables to free graph references\n",
    "        del pred_noise, target_noise, mu, log_var, loss, recon_loss, kl_loss, kld\n",
    "        \n",
    "        # --- MEMORY FIX 3: Clear CUDA cache (for GPU) ---\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update Progress Bar (show unscaled losses)\n",
    "        pbar.set_postfix({\n",
    "            'Recon': f\"{recon_loss_val:.4f}\", \n",
    "            'KL': f\"{kl_loss_val:.4f}\",\n",
    "            'Accum': f\"{(batch_idx + 1) % GRAD_ACCUM_STEPS}/{GRAD_ACCUM_STEPS}\"\n",
    "        })\n",
    "    \n",
    "    # End of Epoch Summary\n",
    "    avg_recon = total_recon_loss / batch_count\n",
    "    avg_kl = total_kl_loss / batch_count\n",
    "    avg_total_loss = avg_recon + avg_kl\n",
    "    print(f\"   Done. Avg Recon: {avg_recon:.4f} | Avg KL: {avg_kl:.4f} | Avg Total Loss: {avg_total_loss:.4f}\")\n",
    "    \n",
    "    # --- EARLY STOPPING CHECK (only after warmup period) ---\n",
    "    if epoch >= EARLY_STOP_WARMUP:  # Start checking after warmup (epochs 0-99 are warmup, check starts at epoch 100)\n",
    "        if avg_total_loss < best_loss:\n",
    "            # Improvement found!\n",
    "            best_loss = avg_total_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_total_loss,\n",
    "            }, best_model_path)\n",
    "            print(f\"   âœ¨ New best loss: {best_loss:.4f} (saved to {best_model_path})\")\n",
    "        else:\n",
    "            # No improvement\n",
    "            patience_counter += 1\n",
    "            print(f\"   â³ No improvement. Patience: {patience_counter}/{EARLY_STOP_PATIENCE}\")\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"\\nğŸ›‘ Early stopping triggered after {epoch + 1} epochs!\")\n",
    "                print(f\"   Best loss: {best_loss:.4f} at epoch {epoch + 1 - patience_counter}\")\n",
    "                print(f\"   Loading best model from {best_model_path}\")\n",
    "                # Load best model\n",
    "                checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                break\n",
    "    \n",
    "    # --- CHECKPOINTING ---\n",
    "    # Save model every 10 epochs (and the first one)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        save_path = os.path.join(CHECKPOINT_DIR, f\"cdvae_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_total_loss,\n",
    "        }, save_path)\n",
    "        print(f\"   ğŸ’¾ Saved checkpoint to {save_path}\")\n",
    "\n",
    "print(\"\\nâœ… Training Complete.\")\n",
    "if best_loss < float('inf'):\n",
    "    print(f\"   Best model saved to: {best_model_path} (loss: {best_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete!\n",
    "\n",
    "Checkpoints are saved in `/kaggle/working/checkpoints/`. These will be available for download after the notebook run completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:40:00.898834Z",
     "iopub.status.busy": "2025-12-23T04:40:00.898069Z",
     "iopub.status.idle": "2025-12-23T04:40:01.035810Z",
     "shell.execute_reply": "2025-12-23T04:40:01.035093Z",
     "shell.execute_reply.started": "2025-12-23T04:40:00.898801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x 2 root root  4096 Dec 23 04:27 checkpoints\n"
     ]
    }
   ],
   "source": [
    "!ls -l | grep \"check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T04:40:24.326844Z",
     "iopub.status.busy": "2025-12-23T04:40:24.326236Z",
     "iopub.status.idle": "2025-12-23T04:40:24.462613Z",
     "shell.execute_reply": "2025-12-23T04:40:24.461712Z",
     "shell.execute_reply.started": "2025-12-23T04:40:24.326806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  cdvae_epoch_10.pt  cdvae_epoch_1.pt\n"
     ]
    }
   ],
   "source": [
    "!cd checkpoints && ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9101353,
     "sourceId": 14262980,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
