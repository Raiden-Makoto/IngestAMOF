{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MOF Crystal Diffusion VAE Training\n",
        "\n",
        "This notebook trains a Crystal Diffusion VAE model on MOF (Metal-Organic Framework) structures.\n",
        "\n",
        "## Setup\n",
        "- Device: CUDA (GPU) on Kaggle\n",
        "- Batch Size: 16 (effective: 128 with gradient accumulation)\n",
        "- Architecture: Crystal Diffusion VAE with GNN encoder/decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add current directory to path for imports (Kaggle structure)\n",
        "if '/kaggle/working' not in sys.path:\n",
        "    sys.path.insert(0, '/kaggle/working')\n",
        "if '.' not in sys.path:\n",
        "    sys.path.insert(0, '.')\n",
        "\n",
        "from utils.dataloader import get_dataloader, MOFDataset, collate_mols\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from model.cdvae import CrystalDiffusionVAE\n",
        "\n",
        "print(\"âœ“ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set hyperparameters and paths for Kaggle environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- HYPERPARAMETERS ---\n",
        "# Hardware / System\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"  # Kaggle working directory\n",
        "\n",
        "# Training Config\n",
        "BATCH_SIZE = 16        # Safe for GPU memory\n",
        "GRAD_ACCUM_STEPS = 8   # Accumulate gradients over 8 batches (effective batch size = 16 * 8 = 128)\n",
        "LR = 1e-3              # Learning Rate\n",
        "EPOCHS = 50           # Total passes through data\n",
        "KL_WEIGHT = 1.0       # Weight for KL Divergence (keeps latent space neat)\n",
        "GRAD_CLIP = 1.0        # Prevents exploding gradients\n",
        "\n",
        "# Model Architecture\n",
        "HIDDEN_DIM = 64\n",
        "LATENT_DIM = 64\n",
        "NUM_LAYERS = 2\n",
        "TIMESTEPS = 1000\n",
        "\n",
        "# Data path (adjust based on your Kaggle dataset structure)\n",
        "DATA_DIR = \"/kaggle/input/processed_graphs\"  # Or \"/kaggle/working/processed_graphs\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)\n",
        "    \n",
        "print(f\"ðŸš€ Starting MOF-Diffusion Training\")\n",
        "print(f\"   Device:      {DEVICE}\")\n",
        "print(f\"   Batch Size:  {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS} with gradient accumulation)\")\n",
        "print(f\"   Latent Dim:  {LATENT_DIM}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Split Data\n",
        "\n",
        "Load the dataset and split into 80% training and 20% test (test is not used for training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data and Split (80% train, 20% test - but we only use train)\n",
        "# Adjust DATA_DIR based on where your processed_graphs folder is located\n",
        "full_dataset = MOFDataset(DATA_DIR)\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, test_dataset = random_split(\n",
        "    full_dataset, \n",
        "    [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        ")\n",
        "\n",
        "# Create dataloader only for training data\n",
        "dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_mols,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"ðŸ“š Data Loaded: {total_size} total crystals\")\n",
        "print(f\"   Training: {train_size} crystals (80%)\")\n",
        "print(f\"   Test: {test_size} crystals (20% - not used for training)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model\n",
        "model = CrystalDiffusionVAE(\n",
        "    hidden_dim=HIDDEN_DIM, \n",
        "    latent_dim=LATENT_DIM, \n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_timesteps=TIMESTEPS\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "print(f\"âœ“ Model initialized on {DEVICE}\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_recon_loss = 0\n",
        "    total_kl_loss = 0\n",
        "    batch_count = 0\n",
        "    \n",
        "    # Progress Bar for this Epoch\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # --- MEMORY FIX 1: Use set_to_none=True to delete grads instead of zeroing ---\n",
        "        optimizer.zero_grad(set_to_none=True)  # SAVES MEMORY: deletes grads instead of zeroing them\n",
        "        # Move Data to Device\n",
        "        atom_types = batch['atom_types'].to(DEVICE)\n",
        "        frac_coords = batch['frac_coords'].to(DEVICE)\n",
        "        lattice = batch['lattice'].to(DEVICE)\n",
        "        mask = batch['mask'].to(DEVICE)\n",
        "        \n",
        "        # --- FORWARD PASS ---\n",
        "        # Model handles encoding, sampling z, adding noise, and decoding\n",
        "        pred_noise, target_noise, mu, log_var = model(\n",
        "            atom_types, frac_coords, lattice, mask\n",
        "        )\n",
        "        \n",
        "        # --- LOSS CALCULATION ---\n",
        "        # 1. Reconstruction Loss (MSE between Predicted Noise and Real Noise)\n",
        "        # We must normalize by the number of valid atoms (sum of mask)\n",
        "        # Note: The output is already masked inside the model, but we double-check mask sum\n",
        "        num_valid_atoms = torch.sum(mask)\n",
        "        recon_loss = F.mse_loss(pred_noise, target_noise, reduction='sum') / (num_valid_atoms + 1e-6)\n",
        "        \n",
        "        # 2. KL Divergence (Regularization)\n",
        "        # Analytical KL for Normal Distributions\n",
        "        # sum(1 + log(var) - mu^2 - var)\n",
        "        kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        # Normalize by batch size and apply weight\n",
        "        kl_loss = (kld / atom_types.size(0)) * KL_WEIGHT\n",
        "        \n",
        "        # Total Loss - scale by accumulation steps to average over accumulated batches\n",
        "        loss = (recon_loss + kl_loss) / GRAD_ACCUM_STEPS\n",
        "        \n",
        "        # --- BACKPROPAGATION (Accumulate Gradients) ---\n",
        "        loss.backward()\n",
        "        \n",
        "        # --- LOGGING ---\n",
        "        # Store loss values before deletion for logging\n",
        "        recon_loss_val = recon_loss.item()\n",
        "        kl_loss_val = kl_loss.item()\n",
        "        total_recon_loss += recon_loss_val\n",
        "        total_kl_loss += kl_loss_val\n",
        "        batch_count += 1\n",
        "        \n",
        "        # Update every GRAD_ACCUM_STEPS batches or at the end of epoch\n",
        "        is_accumulation_step = (batch_idx + 1) % GRAD_ACCUM_STEPS == 0\n",
        "        is_last_batch = (batch_idx + 1) == len(dataloader)\n",
        "        \n",
        "        if is_accumulation_step or is_last_batch:\n",
        "            # Clip Gradients (Crucial for GNN stability)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            \n",
        "            # Take optimizer step (this uses accumulated gradients)\n",
        "            optimizer.step()\n",
        "        \n",
        "        # --- MEMORY FIX 2: DELETE TENSORS ---\n",
        "        # Explicitly delete heavy variables to free graph references\n",
        "        del pred_noise, target_noise, mu, log_var, loss, recon_loss, kl_loss, kld\n",
        "        \n",
        "        # --- MEMORY FIX 3: Clear CUDA cache (for GPU) ---\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Update Progress Bar (show unscaled losses)\n",
        "        pbar.set_postfix({\n",
        "            'Recon': f\"{recon_loss_val:.4f}\", \n",
        "            'KL': f\"{kl_loss_val:.4f}\",\n",
        "            'Accum': f\"{(batch_idx + 1) % GRAD_ACCUM_STEPS}/{GRAD_ACCUM_STEPS}\"\n",
        "        })\n",
        "    \n",
        "    # End of Epoch Summary\n",
        "    avg_recon = total_recon_loss / batch_count\n",
        "    avg_kl = total_kl_loss / batch_count\n",
        "    avg_total_loss = avg_recon + avg_kl\n",
        "    print(f\"   Done. Avg Recon: {avg_recon:.4f} | Avg KL: {avg_kl:.4f} | Avg Total Loss: {avg_total_loss:.4f}\")\n",
        "    \n",
        "    # --- CHECKPOINTING ---\n",
        "    # Save model every 10 epochs (and the first one)\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        save_path = os.path.join(CHECKPOINT_DIR, f\"cdvae_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_total_loss,\n",
        "        }, save_path)\n",
        "        print(f\"   ðŸ’¾ Saved checkpoint to {save_path}\")\n",
        "\n",
        "print(\"\\nâœ… Training Complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Complete!\n",
        "\n",
        "Checkpoints are saved in `/kaggle/working/checkpoints/`. These will be available for download after the notebook run completes.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
